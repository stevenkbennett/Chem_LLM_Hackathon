{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5252f7f-574c-4760-8e28-26b2f552c6a6",
   "metadata": {},
   "source": [
    "# Extracting literature knowledge using LLMs\n",
    "\n",
    "In this task, you will use large language models to extract knowledge from the scientific literature. Your goal is to develop an approach that can answer a series of chemistry and physics exam paper questions.\n",
    "\n",
    "## How to complete this task\n",
    "\n",
    "There are two options to completing this task:\n",
    "\n",
    "1. Using ChatGPT, a fully trained large language model hosted through OpenAI.\n",
    "2. Using paper-qa, a package for answering questions based on PDF or text files.\n",
    "\n",
    "More information on each of these approaches is outlined below.\n",
    "\n",
    "### Option 1: ChatGPT\n",
    "\n",
    "The simplest option is to use ChatGPT to answer the questions. To do this, simply navigate to https://chat.openai.com and create an account. You can ask questions to the large language model directly using the chat box.\n",
    "\n",
    "While directly pasting the question to the model will always get you an answer, in many cases it is unlikely to be the correct one. To achieve better results, you can try \"prompt engineering\". This is adding more information to the prompt (the question) to improve the reliability and accuracy of the results. Research has shown that simply asking the model to respond as if it were an expert can improve the answers given.\n",
    "\n",
    "A quick introduction to prompt engineering is available here: https://www.datacamp.com/tutorial/a-beginners-guide-to-chatgpt-prompt-engineering\n",
    "\n",
    "### Option 2: Paper-qa\n",
    "\n",
    "If you have previous Python programming expertise then we recommend trying the paper-qa approach. [Paper-qa](https://github.com/whitead/paper-qa) is a package for extracting and synthesising information contained in PDF and text files. Under the hood, it uses large language models (like ChatGPT) to:\n",
    "1. Decide which PDF files are relevant to a question.\n",
    "2. Extract the relevant information from PDF files.\n",
    "3. Summarise the extracted information into a final response.\n",
    "\n",
    "A benefit of paper-qa is that it can provide references to where its answer originates from, unlike ChatGPT which can confidently state incorrect information.\n",
    "\n",
    "## The questions\n",
    "\n",
    "The following questions have been taken from past chemistry and physics exam papers. In several cases, they require understanding and summarising different aspects of the subjects, which can make it difficult for a model like ChatGPT.\n",
    "\n",
    "**Please ensure that all answers are less than 100 words. All answers will be truncated to this length when being marked. You can directly instruct the model to provide answers within this word count.**\n",
    "\n",
    "The list of questions is as follows:\n",
    "\n",
    "**Chemistry questions**\n",
    "\n",
    "1. Account for the variation in bond strengths of the Group 17 diatomic molecules (given in kJ mol-1) F2 (158) Cl2 (242) Br2 (192) I2 (151)\n",
    "2. What is the oxidation state and hybridisation of the Cl centres in ClF3 and ClF5?\n",
    "3. Carbon monoxide is a good ligand. Why is the isoelectronic N2 molecule not a good ligand?\n",
    "4. Describe the 1H NMR spectrum of GeH4.\n",
    "5. What is the expected maximum stable oxidation state for (a) Ba, (b) As, (c) Ti, (d) Cr?\n",
    "\n",
    "**Physics questions**\n",
    "\n",
    "6. A\n",
    "7. B\n",
    "8. C\n",
    "9. D\n",
    "10. E\n",
    "\n",
    "## Uploading your results\n",
    "\n",
    "Once you have the list of your 10 answers, you should add them to your GitHub pull request for automated scoring. See the automated scoring documentation for more details on how this process works.\n",
    "\n",
    "Each answer must be on a single line (and not contain any new lines). The answers should be ordered in the same order as above. Accordingly, the file that you upload should only contain 10 lines in total. If your file contains more or less than this, an error message will be shown. An example answers file is shown below:\n",
    "\n",
    "```\n",
    "This is my answer to question 1.\n",
    "This is my answer to question 2.\n",
    "This is my answer to question 3.\n",
    "This is my answer to question 4.\n",
    "This is my answer to question 5.\n",
    "This is my answer to question 6.\n",
    "This is my answer to question 7.\n",
    "This is my answer to question 8.\n",
    "This is my answer to question 9.\n",
    "This is my answer to question 10.\n",
    "```\n",
    "\n",
    "You should name your file: `task2.txt`.\n",
    "\n",
    "## Using paper-qa\n",
    "\n",
    "The rest of this notebook, gives a quick introduction to using paper-qa, and should be used as the starting point for groups following option 2.\n",
    "\n",
    "First, we need to download and install the necessary packages to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e4e9ae-ce94-4102-8107-84c0ff845048",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install paper-qa openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe0c2ee-5978-44d0-b9c8-d0e8783a93f4",
   "metadata": {},
   "source": [
    "The next step is adding your OpenAI API key. This is necessary for paper-qa to formulate responses to the prompts, and to enable extraction of literature information. You should recieve your group's personalised API key from the hackathon organisers.\n",
    "\n",
    "**Each group has a fixed budget for API requests. Adding new documents, and asking more questions will each generate multiple requests, so be mindful when using the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bdfa07-e539-44d4-a883-973426cbd448",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"PUT_API_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d2c9b-837c-402f-86ec-f5522dd58466",
   "metadata": {},
   "source": [
    "Next, we set up paper-qa for use in notebook mode. It is essential that you run this code, otherwise the rest of the notebook will not work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e363a326-d7ef-4d2c-aa8c-b18329d64438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aa62a1-9325-4c32-9f18-725080ad4567",
   "metadata": {},
   "source": [
    "### Loading the Docs object and adding documents\n",
    "\n",
    "First, we load a pre-prepared paperqa `Docs` object. See the [paper-qa documentation](https://github.com/whitead/paper-qa/tree/main#usage) for more details on this object. It is recommended that you use this docs object as the starting point for your queries.\n",
    "\n",
    "This object has already been configured to include the following textbooks:\n",
    "\n",
    "1. Inorganic Chemistry (2014) *Shriver, Weller, Overton, Rourke, Armstrong*, 6th Ed.\n",
    "\n",
    "For reference, the object was created using the following code:\n",
    "\n",
    "```python\n",
    "from paperqa import Docs\n",
    "\n",
    "docs = Docs(llm='gpt-3.5-turbo')\n",
    "docs.add(\n",
    "    \"Inorganic Chemistry.pdf\", \n",
    "    citation=\"Inorganic Chemistry, Shriver, 2014\"\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e702f-813f-4ecd-a238-8997e33dc7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"docs.p\", \"rb\") as f:\n",
    "    docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434d706-bfca-4845-91d7-5dec37a055e5",
   "metadata": {
    "tags": []
   },
   "source": [
    "New documents (PDFs and text files) can be added to the `docs` object using the following code:\n",
    "\n",
    "```python\n",
    "docs.add (\"my_file.pdf\")\n",
    "```\n",
    "\n",
    "You should add any documents that you think will help answer the questions. These can be papers taken from the scientific literature, text from websites, or any other sources you see fit.\n",
    "\n",
    "### Querying the text corpus\n",
    "\n",
    "You can query the `docs` object to help answer questions. When you do so, paper-qa will perform the following task:\n",
    "\n",
    "1. Search all documents for the top 10 relevant passages to the query (using ChatGPT).\n",
    "2. Create summary of each passage relevant to the query (using ChatGPT).\n",
    "3. Put the summaries into a context.\n",
    "4. Generate an answer taking into account the context (using ChatGPT).\n",
    "\n",
    "An example of using the `docs` object is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30bc83e-ad0f-45ff-bb41-7c265555e46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = docs.query(\"What is an oxidation state?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3463d267-df03-4cdf-af7a-6490962520be",
   "metadata": {},
   "source": [
    "You can inspect the context (the selected passages) that paper-qa found relevant to your query using the `context` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987e4af-3172-4405-86fc-36008b24b04d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(answer.context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b47016e-de65-47ce-b2ef-d8f1c4d0708d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Customising prompts\n",
    "\n",
    "Steps 1, 2, and 4 outlined above each use ChatGPT to extract information. Each step uses a custom prompt to achieve its goal. All of these prompts are configurable in paper-qa.\n",
    "\n",
    "Below, we have reproduced the prompts that paper-qa uses. If you edit the cell, the prompts will be updated and you can tune how information is extracted. This can be an effective way of extracting more information for your query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d842361e-f829-4b27-852f-1468bd0fe8ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from paperqa.prompts import _get_datetime\n",
    "from paperqa.types import PromptCollection\n",
    "\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"citation\", \"question\", \"summary_length\"],\n",
    "    template=\"Summarize the text below to help answer a question. \"\n",
    "    \"Do not directly answer the question, instead summarize \"\n",
    "    \"to give evidence to help answer the question. \"\n",
    "    'Reply \"Not applicable\" if text is irrelevant. '\n",
    "    \"Use {summary_length}. At the end of your response, provide a score from 1-10 on a newline \"\n",
    "    \"indicating relevance to question. Do not explain your score. \"\n",
    "    \"\\n\\n\"\n",
    "    \"{text}\\n\\n\"\n",
    "    \"Excerpt from {citation}\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Relevant Information Summary:\",\n",
    ")\n",
    "\n",
    "qa_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"answer_length\", \"question\"],\n",
    "    template=\"Write an answer ({answer_length}) \"\n",
    "    \"for the question below based on the provided context. \"\n",
    "    \"If the context provides insufficient information, \"\n",
    "    'reply \"I cannot answer\". '\n",
    "    \"For each part of your answer, indicate which sources most support it \"\n",
    "    \"via valid citation markers at the end of sentences, like (Example2012). \"\n",
    "    \"Answer in an unbiased, comprehensive, and scholarly tone. \"\n",
    "    \"If the question is subjective, provide an opinionated answer in the concluding 1-2 sentences. \\n\\n\"\n",
    "    \"{context}\\n\"\n",
    "    \"Question: {question}\\n\"\n",
    "    \"Answer: \",\n",
    ")\n",
    "\n",
    "select_paper_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"papers\"],\n",
    "    template=\"Select papers that may help answer the question below. \"\n",
    "    \"Papers are listed as $KEY: $PAPER_INFO. \"\n",
    "    \"Return a list of keys, separated by commas. \"\n",
    "    'Return \"None\", if no papers are applicable. '\n",
    "    \"Choose papers that are relevant, from reputable sources, and timely \"\n",
    "    \"(if the question requires timely information). \\n\\n\"\n",
    "    \"Question: {question}\\n\\n\"\n",
    "    \"{papers}\\n\\n\"\n",
    "    \"Selected keys:\",\n",
    ")\n",
    "\n",
    "citation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"Provide the citation for the following text in MLA Format. Today's date is {date}\\n\"\n",
    "    \"{text}\\n\\n\"\n",
    "    \"Citation:\",\n",
    "    partial_variables={\"date\": _get_datetime},\n",
    ")\n",
    "\n",
    "docs.prompts = PromptCollection(\n",
    "    summary=summary_prompt,\n",
    "    qa=qa_prompt,\n",
    "    select=select_paper_prompt,\n",
    "    cite=citation_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d550a-fb2e-4bcf-8713-d6487c3c5348",
   "metadata": {},
   "source": [
    "Any new queries to the docs objects will use the updated prompt.\n",
    "\n",
    "### Querying ChatGPT and other OpenAI LLMs\n",
    "\n",
    "You may find that paper-qa is too restrictive. If you want to query ChatGPT directly you can use the `OpenAI` object from the `langchain` package.\n",
    "\n",
    "Below, we create a model to query the `text-davinci-003` OpenAI model. This is similar to ChatGPT but is less conversational. More information on the models available in OpenAI can be found on the OpenAI [documentation page](https://platform.openai.com/docs/models).\n",
    "\n",
    "The `temperature` parameter adjusts the randomness of the output. Higher values like 0.9 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e86783-a727-441d-a792-1fc6e6835ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import OpenAI\n",
    "\n",
    "model = OpenAI(model_name=\"text-davinci-003\", temperature=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227f3067-e178-4d39-873e-f3c1ff2cb29e",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can query the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e158ac5-afe5-4498-a1aa-d402253ca760",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = model(\"What is an oxidation state?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7e4661-d2e7-43dc-81ac-4e91f1653aa2",
   "metadata": {},
   "source": [
    "Directly using OpenAI models may give you answers where paper-qa may not. However, OpenAI models are less strict about providing correct information, so beware of the results.\n",
    "\n",
    "### Prompt engineering\n",
    "\n",
    "ChatGPT, paper-qa, and OpenAI models can all be tuned using prompt engineering. It may be better to ask your question in multiple parts, to state the expected audience of your question, or to ask the model to respond as an expert. A quick introduction to prompt engineering is available here: https://www.datacamp.com/tutorial/a-beginners-guide-to-chatgpt-prompt-engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
